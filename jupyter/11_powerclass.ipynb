{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful clasifiers \n",
    "Here we will try powerful classifiers, including support vector machines, random forest, neural networks. Remember that 90% of Machine Learning is about classification. This lecture includes precise but uninterpretable classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file\n",
    "Commonly two libraries are used to load a csv files.\n",
    "- numpy function `np.loadtext` and `np.genfromtext ` \n",
    "- pandas function `pd.read_csv`\n",
    "\n",
    "Here we prefer using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path='data/'\n",
    "filename = path+'spamdata.csv'\n",
    "spam = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam.values[:,:57]\n",
    "y = spam.values[:,57]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random forest is one of the powerful classification tools. Computations for moderate number of samples is rather fast. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566160520607375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(rf.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppor Vector Machines\n",
    "SVMs are like linear regression, expanded in kernel space. Their behaviour is similar to local regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try C=1, C=10, C=100\n",
    "from sklearn.svm import SVC\n",
    "sv = SVC(C=10)\n",
    "sv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8872017353579176"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sv.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7594001304064334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only one iteration of KFold for single C\n",
    "from sklearn.model_selection import KFold\n",
    "k = 5\n",
    "acck = np.zeros(k)\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "i = 0\n",
    "for train_i, test_i in kf.split(spam):\n",
    "    sv = SVC(C=0.1)\n",
    "    sv = sv.fit(X[train_i], y[train_i])\n",
    "    acck[i]=accuracy_score(sv.predict(X[test_i]), y[test_i], normalize=False)\n",
    "    i+=1\n",
    "np.sum(acck)/X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75157574, 0.76374701])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B iteration of k-fold for single C\n",
    "k = 5\n",
    "acck = np.zeros(k)\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "B = 2\n",
    "acc = np.zeros(B)\n",
    "for j in range(B):    \n",
    "    i = 0\n",
    "    for train_i, test_i in kf.split(spam):\n",
    "        sv = SVC(C=0.1)\n",
    "        sv = sv.fit(X[train_i], y[train_i])\n",
    "        acck[i]=accuracy_score(sv.predict(X[test_i]), y[test_i], normalize=False)\n",
    "        i+=1\n",
    "    acc[j] = np.sum(acck)\n",
    "acc/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2788., 3946., 3947., 3891., 3917.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One iteration of Kfold and tune C between zero and 10\n",
    "nb = 5\n",
    "penalty = np.linspace(0.01, 100, nb)\n",
    "k = 5\n",
    "acck = np.zeros(k)\n",
    "acc = np.zeros(len(penalty))\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "for c in range(len(penalty)):\n",
    "    i = 0\n",
    "    for train_i, test_i in kf.split(spam):\n",
    "        sv = SVC(C=penalty[c])\n",
    "        sv = sv.fit(X[train_i], y[train_i])\n",
    "        acck[i]=accuracy_score(sv.predict(X[test_i]), y[test_i], normalize=False)\n",
    "        i+=1\n",
    "    acc[c] = np.sum(acck)\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B iteration of Kfold and tune C between zero and 10\n",
    "# Leave it to you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill  it here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Shallow neural networks often produces comparable results with random forest, bagging and boosting. Deep neural netoworks for large data requires load tensorflow, which we do not discuss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build two hidden layers, each layer with 10 neurons\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes=(10, 10), activation='logistic')\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544468546637744"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(nn.predict(X_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
